#!/usr/bin/env python3
"""STEP 1: Cascade Analytics Data Collection Script

This Python script collects Cascade's feature usage and impact data (suggested lines versus 
lines accepted) per user by making API calls to the Codeium analytics endpoint.

WORKFLOW:
1. Loads SERVICE_KEY from .env file for authentication
2. Loads email addresses from email_api_mapping_YYYY-MM-DD.json (generated by AnalyticScripts.email_api_mapping)
   or uses a default list if the file doesn't exist
3. Makes one API call per email to https://server.codeium.com/api/v1/CascadeAnalytics
4. Queries three types of data for each user:
   - cascade_lines: Lines suggested vs accepted
   - cascade_runs: Model usage, messages sent, prompts used
   - cascade_tool_usage: Tool usage statistics (CODE_ACTION, VIEW_FILE, etc.)
5. Saves all results to cascade_analytics_results.json

NEXT STEP:
Run generate_csv_reports.py to parse the JSON output and create structured CSV reports.

CONFIGURATION:
- Date range: Default is the last month, customizable via command-line arguments
- Email list: Generated by AnalyticScripts.email_api_mapping or modify the 'default_emails' list in main() function
- Service key: Must be set in .env file as SERVICE_KEY=your_key_here
"""

import os
import requests
import json
import datetime
import argparse
from dotenv import load_dotenv
from typing import List, Dict, Any

# Define output directory
OUTPUT_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'output')

# Ensure output directory exists
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Load environment variables from .env file
load_dotenv()

def get_cascade_analytics(email: str, start_timestamp: str, end_timestamp: str) -> Dict[Any, Any]:
    """
    Get Cascade analytics for a specific email address.
    
    Args:
        email: The email address to query
        start_timestamp: Start time in ISO format (e.g., "2025-01-01T00:00:00Z")
        end_timestamp: End time in ISO format (e.g., "2025-01-02T00:00:00Z")
    
    Returns:
        Dictionary containing the API response
    """
    service_key = os.getenv('SERVICE_KEY')
    if not service_key:
        raise ValueError("SERVICE_KEY not found in environment variables")
    
    url = "https://server.codeium.com/api/v1/CascadeAnalytics"
    
    payload = {
        "service_key": service_key,
        "start_timestamp": start_timestamp,
        "end_timestamp": end_timestamp,
        "emails": [email],
        "query_requests": [
            {
                "cascade_lines": {}
            },
            {
                "cascade_runs": {}
            },
            {
                "cascade_tool_usage": {}
            }
        ]
    }
    
    headers = {
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.post(url, json=payload, headers=headers)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Error making request for {email}: {e}")
        return {"error": str(e), "email": email}

def process_multiple_emails(emails: List[str], start_timestamp: str, end_timestamp: str) -> Dict[str, Any]:
    """
    Process multiple emails and get Cascade analytics for each.
    
    Args:
        emails: List of email addresses to process
        start_timestamp: Start time in ISO format
        end_timestamp: End time in ISO format
    
    Returns:
        Dictionary with results for each email
    """
    results = {}
    
    for email in emails:
        print(f"Processing analytics for: {email}")
        result = get_cascade_analytics(email, start_timestamp, end_timestamp)
        results[email] = result
        
    return results

def load_emails_from_file(filename: str = "unique_emails.json") -> List[str]:
    """
    Load email addresses from a JSON file.
    
    Args:
        filename: Path to the JSON file containing email addresses
        
    Returns:
        List of email addresses
    """
    try:
        with open(filename, 'r') as f:
            data = json.load(f)
            # Check if the data is a dictionary (emails as keys) or a list
            if isinstance(data, dict):
                emails = list(data.keys())
            else:
                emails = data
            print(f"Loaded {len(emails)} email addresses from {filename}")
            return emails
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"Error loading emails from {filename}: {e}")
        return []

def parse_date(date_str: str) -> datetime.datetime:
    """
    Parse a date string in YYYY-MM-DD format.
    
    Args:
        date_str: Date string in YYYY-MM-DD format
        
    Returns:
        datetime object
    """
    try:
        return datetime.datetime.strptime(date_str, "%Y-%m-%d")
    except ValueError:
        raise ValueError(f"Invalid date format: {date_str}. Please use YYYY-MM-DD format.")

def get_default_start_date() -> datetime.datetime:
    """
    Get the default start date (1 month ago).
    
    Returns:
        datetime object representing 1 month ago
    """
    now = datetime.datetime.now()
    # Go back 1 month (approximately 30 days)
    return now - datetime.timedelta(days=30)

def main():
    """
    Main function to run the Cascade analytics for multiple users.
    """
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Collect Cascade analytics data for users')
    parser.add_argument('--limit', type=int, default=0, 
                        help='Limit the number of emails to process (0 for all emails)')
    parser.add_argument('--email', type=str, 
                        help='Process a single specific email')
    parser.add_argument('--start-date', type=str, 
                        help='Start date in YYYY-MM-DD format (default: 1 month ago)')
    parser.add_argument('--end-date', type=str, 
                        help='End date in YYYY-MM-DD format (default: today)')
    parser.add_argument('--output', type=str,
                        help='Output JSON file name (default: cascade_analytics_results_YYYY-MM-DD.json)')
    args = parser.parse_args()
    
    # Default email list - used only if unique_emails.json is not found
    default_emails = [
        "vanessa.salas@codeium.com",
        # Add more emails here as needed
    ]
    
    # Try to load emails from email_api_mapping files
    # First try the most recent email_api_mapping file
    import glob
    mapping_files = sorted(glob.glob("email_api_mapping_*.json"), key=os.path.getmtime, reverse=True)
    
    if mapping_files:
        emails_from_file = load_emails_from_file(mapping_files[0])
    else:
        # Fall back to unique_emails_short.json for backward compatibility
        emails_from_file = load_emails_from_file("unique_emails_short.json")
    
    # Use emails from file if available, otherwise use default list
    emails = emails_from_file if emails_from_file else default_emails
    
    if not emails:
        print("No emails found. Please run 'python -m AnalyticScripts.email_api_mapping' first or add emails to the default_emails list.")
        return
    
    # Process a single email if specified
    if args.email:
        if args.email in emails:
            emails = [args.email]
            print(f"Processing only the specified email: {args.email}")
        else:
            print(f"Warning: Specified email {args.email} not found in the list. Processing all emails.")
    
    # Limit the number of emails if specified
    if args.limit > 0 and args.limit < len(emails):
        emails = emails[:args.limit]
        print(f"Limiting to the first {args.limit} emails")
    
    # Set time range
    now = datetime.datetime.now()
    
    # Process start date
    if args.start_date:
        start_date = parse_date(args.start_date)
    else:
        start_date = get_default_start_date()
    
    # Process end date
    if args.end_date:
        end_date = parse_date(args.end_date)
    else:
        end_date = now
    
    # Convert to ISO format with UTC timezone
    start_timestamp = start_date.strftime("%Y-%m-%dT00:00:00Z")
    end_timestamp = end_date.strftime("%Y-%m-%dT23:59:59Z")
    
    print(f"Starting Cascade Analytics processing for {len(emails)} users...")
    print(f"Date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}")
    
    results = process_multiple_emails(emails, start_timestamp, end_timestamp)
    
    # Print results
    print("\n" + "="*50)
    print("RESULTS SUMMARY")
    print("="*50)
    
    for email, result in results.items():
        print(f"\nEmail: {email}")
        if "error" in result:
            print(f"  Error: {result['error']}")
        else:
            print(f"  Status: Success")
            print(f"  Response keys: {list(result.keys())}")
    
    # Determine output file path
    output_file = args.output
    if not output_file:
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d")
        output_file = os.path.join(OUTPUT_DIR, f"cascade_analytics_results_{timestamp}.json")
    
    # Save results to a file
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"\nResults saved to {output_file}")

if __name__ == "__main__":
    main()